[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Compress your AI model",
    "section": "",
    "text": "Compress your AI model\nwith tinyMLaaS\nUpload your AI model on device for efficient run to save your costs, time, and cloud space\n\nGet Started See Demo\n\n\n\n\n\n\nRun by NinjaLABO.\nNext\nnext"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "IREE review\n\n\n\n\n\nSome description\n\n\n\n\n\nNov 7, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying PyTorch Static Quantization\n\n\n\n\n\nA deep dive into how PyTorch performs inference with quantized models.\n\n\n\n\n\nAug 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, theme):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=theme, legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86, \"plotly_dark\")\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on x86\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.552532\n469.382812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.451820\n65.339844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n72.082257\n33.976562\n11.949406\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, \"plotly_white\")\n\n\n\n\n                                                \n\n\nFigure 2: Performance comparison on ARM\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n7.563920\n462.953125\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n6.604064\n78.375000\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n12.067646\n39.828125\n11.949406",
    "crumbs": [
      "Test",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "hello.html#x86",
    "href": "hello.html#x86",
    "title": "Quarto Basics",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, theme):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),height=600, width=900,\n        template=theme, legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86, \"plotly_dark\")\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on x86\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.552532\n469.382812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n10.451820\n65.339844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n72.082257\n33.976562\n11.949406",
    "crumbs": [
      "Test",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "hello.html#arm",
    "href": "hello.html#arm",
    "title": "Quarto Basics",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, \"plotly_white\")\n\n\n\n\n                                                \n\n\nFigure 2: Performance comparison on ARM\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n7.563920\n462.953125\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n6.604064\n78.375000\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n12.067646\n39.828125\n11.949406",
    "crumbs": [
      "Test",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;, 2nd July, 2024"
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "The Need for AI Compression",
    "text": "The Need for AI Compression\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "Benefits of AI Compression as-a-Service",
    "text": "Benefits of AI Compression as-a-Service\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "TinyML as-a-Service: Empowering the Internet of Things",
    "text": "TinyML as-a-Service: Empowering the Internet of Things\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\nKey Features of TinyMLaaS\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "Applications and Use Cases",
    "text": "Applications and Use Cases\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "Conclusion",
    "text": "Conclusion\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "NinjaLABO is specialized at compressing AI models (DNN models) along with optimized tinyRuntime via our SaaS platform.",
    "crumbs": [
      "Features",
      "About"
    ]
  },
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "IREE Review\nImportant review…"
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/test.html",
    "href": "blogs/test.html",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "",
    "text": "In the world of machine learning, optimizing model performance and efficiency is crucial, especially for deploying models on edge devices with limited resources. One powerful technique to achieve this is quantization, which reduces the precision of the numbers used in a model’s computations. PyTorch supports two types of quantization: dynamic and static. Dynamic quantization adjusts the precision of weights at runtime, while static quantization involves converting the model’s weights and activations to lower precision based on calibration data. This article will focus on statically quantized models, breaking down the core concepts and steps involved in PyTorch’s approach to inference with these models.\nNote: This article assumes you are already familiar with quantization, particularly static quantization. If not, I recommend checking out the some materials, e.g., our technology page for an introduction.\nfrom fastai.vision.all import *\n\nimport torch\nfrom torch.ao.quantization import get_default_qconfig_mapping\nimport torch.ao.quantization.quantize_fx as quantize_fx\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\nLet’s start by creating a Quantizer class to quantize a PyTorch model. For an introduction to PyTorch quantization, you can refer to the official documentation. As an example, I will use the Imagenette2-320 dataset and the ResNet18 model. For convenience, I will leverage the Fastai learner to streamline this process.\nclass Quantizer():\n    def __init__(self, backend=\"x86\"):\n        self.qconfig = get_default_qconfig_mapping(backend)\n        torch.backends.quantized.engine = backend\n\n    def quantize(self, model, calibration_dls):\n        x, _ = calibration_dls.valid.one_batch()\n        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n        with torch.no_grad():\n            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n\n        return model_prepared, convert_fx(model_prepared)\npath = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\ndls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\nlearn = vision_learner(dls, resnet18)\nmodel_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)\nIn static quantization, the scaling factors and zero points for weights and activations are determined after model calibration but before inference. In this context, we are using per-tensor quantization, which means that there is a single scaling factor and zero point applied uniformly across all elements in each tensor of a layer. This approach is straightforward and computationally efficient, as it simplifies the quantization process by treating the entire tensor as a whole.\nIn the above cell, model_prepared instance represents the model after it has recorded the range of activations across a validation dataset. This model contains the necessary information about the model structure and activation ranges, from which the scaling factors and zero points are calculated. Below is an example of the quantization parameters for some activations. The HistogramObserver is used to record the activation ranges. The first output shows the quantized parameters of the first activation, which is the model input, while the second output shows the quantization parameters of the second activation, which is the output of the first Conv2d + ReLU layer. In PyTorch, to avoid redundant quantization and dequantization processes between layers, batch normalization is folded into the preceding layer (batch normalization folding), and the ReLU layer is fused with the layer it follows.\n# Example activation quantization parameters\nfor i in range(3):\n    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n    scale, zero_p = attr.calculate_qparams()\n    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))\n\nHistogramObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\nScaling Factor: 0.018649335950613022\nZero Point: 114\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\nqmodel instance represents the quantized model. It contains quantized weights, along with their associated scaling factor and zero point, as well as the scaling factor and zero point for activations. Additionally, it includes some non-quantized parameters, which I will explain later.\nqmodel\n\nGraphModule(\n  (0): Module(\n    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008901300840079784, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024013830348849297, zero_point=149, padding=(1, 1))\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007031331304460764, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031252723187208176, zero_point=156, padding=(1, 1))\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007301042787730694, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019116230309009552, zero_point=124, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.01664934679865837, zero_point=135)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008282394148409367, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02566305175423622, zero_point=137, padding=(1, 1))\n      )\n    )\n    (6): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010484358295798302, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02675902470946312, zero_point=90, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008271278813481331, zero_point=162)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00832998938858509, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.027811763808131218, zero_point=142, padding=(1, 1))\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006999513134360313, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.023119885474443436, zero_point=140, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.02033478580415249, zero_point=128)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.006345659960061312, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12105856835842133, zero_point=88, padding=(1, 1))\n      )\n    )\n  )\n  (1): Module(\n    (0): Module(\n      (mp): AdaptiveMaxPool2d(output_size=1)\n      (ap): AdaptiveAvgPool2d(output_size=1)\n    )\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): QuantizedDropout(p=0.25, inplace=False)\n    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.08005672693252563, zero_point=0, qscheme=torch.per_tensor_affine)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): QuantizedDropout(p=0.5, inplace=False)\n    (8): QuantizedLinear(in_features=512, out_features=10, scale=0.10456003248691559, zero_point=150, qscheme=torch.per_tensor_affine)\n  )\n)\nLet’s investigate the first layer of qmodel, i.e., quantized Conv2d + ReLU layer.\nlayer = qmodel._modules['0']._modules['0']\nprint(layer)\nprint(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n                                                       layer.weight().q_zero_point()))\nprint(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n                                                                  layer.zero_point))\n\nprint(\"Example weights:\", layer.weight()[0, 0, 0])\nprint(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())\n\nQuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\nWeight Scale: 0.0030892190989106894, Weight Zero Point: 0\nOutput Scaling Factor: 0.011327190324664116, Output Zero Point: 0\n\nExample weights: tensor([-0.0031,  0.0000,  0.0000,  0.0185,  0.0124,  0.0031, -0.0031],\n       size=(7,), dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n       scale=0.0030892190989106894, zero_point=0)\nIn integer representation: tensor([-1,  0,  0,  6,  4,  1, -1], dtype=torch.int8)\nAs shown above, the quantized layer contains two scaling factors and zero points: one for the weights and another for the output activation. You may have noticed that the output scaling factor and zero point are the same as those displayed in the second cell above, as they represent the same activation.\nWhat about biases, which I haven’t discussed yet? In PyTorch, biases are not quantized during the initial quantization stage. Instead, they are quantized at inference. Although bias quantization could technically be performed at the same stage as weight quantization, PyTorch does not display the quantized biases at this point. The formula for bias quantization in PyTorch is \\[\nb_q = round(b / (si * sw))\n\\] , where \\(b_q\\) is quantized bias, \\(b\\) is bias before quantization, \\(si\\) is input activation scale and \\(sw\\) is weight scale. For more details, you can refer to this discussion.\nIn addition, the model may include other non-quantized parameters, such as parameters in batch normalization layers that are not fused. This is likely because quantizing the activations in these layers would not provide significant benefits."
  },
  {
    "objectID": "blogs/test.html#what-happens-during-inference",
    "href": "blogs/test.html#what-happens-during-inference",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "What happens during inference?",
    "text": "What happens during inference?\nThis section demonstrates how calculations are performed in the quantized model during inference. To illustrate this, I calculate the output of the first convolutional layer and validate it against the actual result.\n\nlayer_input = None\nlayer_output = None\n\ndef hook_fn(module, input, output):\n    global layer_output, layer_input\n    layer_input = input\n    layer_output = output\n\nimg = torch.rand([1, 3, 224, 224])\nhook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\noutput = qmodel(img)\nhook.remove()\nprint(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\nprint(\"Example output:\", layer_output[0,0,0,:10].int_repr())\n\nExample input: tensor([163, 119, 155, 138, 126, 164, 115, 132, 115, 166], dtype=torch.uint8)\nExample output: tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.uint8)\n\n\n\nimport numpy as np\n\ndef quantize(x, qparams, itype):\n    xtype = torch.iinfo(itype)\n    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n\ndef dequantize(x, qparams):\n    return (x - qparams[1]) * qparams[0]\n\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    N, C, H, W = input_data.shape\n    out_h = (H + 2 * pad - filter_h) // stride + 1\n    out_w = (W + 2 * pad - filter_w) // stride + 1\n\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride * out_h\n        for x in range(filter_w):\n            x_max = x + stride * out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n    return torch.tensor(col)\n\n# first use im2col, which is efficient way to perform Conv2d operation\ninp = im2col(img, 7, 7, 2, 3).float()\n# quantize input values using input scale and zero point\ninp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n# get quantized weights, weight scale and quantize biases\nw = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\nsw = qmodel._modules['0']._modules['0'].weight().q_scale()\nb = quantize(qmodel._modules['0']._modules['0'].bias(),\n             [layer_input[0].q_scale() * sw, 0], torch.int32)\nb = b.reshape(1,64,1,1).detach()\n# calculate matmul in Conv2d and add biases\nout = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n# dequantize, perform ReLU and quantize based on output scale and zero point\nout = out * sw * layer_input[0].q_scale()\nout = torch.relu(out)\nout = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)\n\n\ntorch.allclose(out, layer_output.int_repr().float())\nprint(\"Output: \", out[0, 0, 0, :10])\n\nOutput:  tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n\nOur calculation matches the actual result, which is a good sign. Although some operations in PyTorch’s implementation might be performed in a different order, the overall process is likely very similar."
  }
]